{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    OpenAITextEmbedding\n",
    ")\n",
    "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.connectors.memory.astradb import AstraDBMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"aboutMe\"\n",
    "\n",
    "async def populate_memory(memory: SemanticTextMemory) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await memory.save_information(collection_id, id=\"info1\", text=\"My name is Andrea\")\n",
    "    await memory.save_information(collection_id, id=\"info2\", text=\"I currently work as a tour guide\")\n",
    "    await memory.save_information(collection_id, id=\"info3\", text=\"I've been living in Seattle since 2005\")\n",
    "    await memory.save_information(collection_id, id=\"info4\", text=\"I visited France and Italy five times since 2015\")\n",
    "    await memory.save_information(collection_id, id=\"info5\", text=\"My family is from New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search the populated memory store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_memory_examples(memory: SemanticTextMemory) -> None:\n",
    "    questions = [\n",
    "        \"what's my name\",\n",
    "        \"where do I live?\",\n",
    "        \"where's my family from?\",\n",
    "        \"where have I traveled?\",\n",
    "        \"what do I do for work\",\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        result = await memory.search(\"aboutMe\", question)\n",
    "        print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a chat application that uses the populated Astra DB Serverless vector store as context for your queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_chat_with_memory(\n",
    "    kernel: sk.Kernel,\n",
    "    service_id: str,\n",
    ") -> sk.KernelFunction:\n",
    "    \n",
    "    fact1 = \"what is my name?\"\n",
    "    fact2 = \"where do I live?\"\n",
    "    fact3 = \"where's my family from?\"\n",
    "    fact4 = \"where have I traveled?\"\n",
    "    fact5 = \"what do I do for work?\"\n",
    "\n",
    "    sk_prompt = \"\"\"\n",
    "    ChatBot can have a conversation with you about any topic.\n",
    "    It can give explicit instructions or say 'I don't know' if\n",
    "    it does not have an answer.\n",
    "\n",
    "    Information about me, from previous conversations:\n",
    "    - {{$fact1}} {{recall $fact1}}\n",
    "    - {{$fact2}} {{recall $fact2}}\n",
    "    - {{$fact3}} {{recall $fact3}}\n",
    "    - {{$fact4}} {{recall $fact4}}\n",
    "    - {{$fact5}} {{recall $fact5}}\n",
    "\n",
    "    Chat:\n",
    "    {{$chat_history}}\n",
    "    User: {{$user_input}}\n",
    "    ChatBot: \"\"\".strip()\n",
    "\n",
    "    prompt_template_config = PromptTemplateConfig(\n",
    "        template=sk_prompt,\n",
    "        execution_settings={\n",
    "            service_id: kernel.get_service(service_id).get_prompt_execution_settings_class()(service_id=service_id)\n",
    "        },\n",
    "    )\n",
    "    chat_func = kernel.create_function_from_prompt(\n",
    "        function_name=\"chat_with_memory\",\n",
    "        plugin_name=\"chat\",\n",
    "        prompt_template_config=prompt_template_config,\n",
    "    )\n",
    "    return chat_func\n",
    "    \"\"\"\n",
    "    chat_func = kernel.create_semantic_function(sk_prompt, max_tokens=200, temperature=0.8)\n",
    "\n",
    "    context = kernel.create_new_context()\n",
    "    context[\"fact1\"] = \"what is my name?\"\n",
    "    context[\"fact2\"] = \"where do I live?\"\n",
    "    context[\"fact3\"] = \"where's my family from?\"\n",
    "    context[\"fact4\"] = \"where have I traveled?\"\n",
    "    context[\"fact5\"] = \"what do I do for work?\"\n",
    "\n",
    "    context[sk.core_plugins.TextMemoryPlugin.COLLECTION_PARAM] = \"aboutMe\"\n",
    "    context[sk.core_plugins.TextMemoryPlugin.RELEVANCE_PARAM] = \"0.8\"\n",
    "\n",
    "    context[\"chat_history\"] = \"\"\n",
    "\n",
    "    return chat_func, context\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with the memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(kernel: sk.Kernel, chat_func: sk.KernelFunction) -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    answer = await kernel.invoke(chat_func, request=user_input)\n",
    "\n",
    "    print(f\"ChatBot:> {answer}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the main function to perform the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "\n",
    "chat_service_id = \"chat\"\n",
    "oai_chat_service = OpenAIChatCompletion(service_id=chat_service_id, ai_model_id=\"gpt-3.5-turbo\", api_key=api_key, org_id=org_id)\n",
    "embedding_gen = OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id)\n",
    "\n",
    "kernel.add_service(oai_chat_service)\n",
    "kernel.add_service(embedding_gen)\n",
    "\n",
    "#original volatile memorystore instance\n",
    "#kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "#kernel.import_plugin(sk.core_plugins.TextMemoryPlugin(), \"TextMemoryPlugin\")\n",
    "\n",
    "app_token, db_id, region, keyspace = sk.astradb_settings_from_dot_env()\n",
    "astra_store = AstraDBMemoryStore(app_token, db_id, region, keyspace, 1536, \"cosine\")\n",
    "\n",
    "memory = SemanticTextMemory(storage=astra_store, embeddings_generator=embedding_gen)\n",
    "kernel.import_plugin_from_object(TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Populating memory...\")\n",
    "await populate_memory(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asking questions... (manually)\")\n",
    "await search_memory_examples(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting function \"chat_with_memory\" in collection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up a chat (with memory!)\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up a chat (with memory!)\")\n",
    "chat_func = await setup_chat_with_memory(kernel, chat_service_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin chatting (type 'exit' to exit):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Variable `$: fact1` not found in the KernelArguments\n",
      "Variable `$: fact1` not found in the KernelArguments\n",
      "Error occurred while invoking function recall: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.OpenAITextEmbedding'> service failed to generate embeddings\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"\\'$.input\\' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}'))\n",
      "Variable `$: fact2` not found in the KernelArguments\n",
      "Variable `$: fact2` not found in the KernelArguments\n",
      "Error occurred while invoking function recall: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.OpenAITextEmbedding'> service failed to generate embeddings\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"\\'$.input\\' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}'))\n",
      "Variable `$: fact3` not found in the KernelArguments\n",
      "Variable `$: fact3` not found in the KernelArguments\n",
      "Error occurred while invoking function recall: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.OpenAITextEmbedding'> service failed to generate embeddings\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"\\'$.input\\' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}'))\n",
      "Variable `$: fact4` not found in the KernelArguments\n",
      "Variable `$: fact4` not found in the KernelArguments\n",
      "Error occurred while invoking function recall: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.OpenAITextEmbedding'> service failed to generate embeddings\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"\\'$.input\\' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}'))\n",
      "Variable `$: fact5` not found in the KernelArguments\n",
      "Variable `$: fact5` not found in the KernelArguments\n",
      "Error occurred while invoking function recall: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.OpenAITextEmbedding'> service failed to generate embeddings\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"\\'$.input\\' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}'))\n",
      "Variable `$: chat_history` not found in the KernelArguments\n",
      "Variable `$: user_input` not found in the KernelArguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:> Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin chatting (type 'exit' to exit):\\n\")\n",
    "chatting = True\n",
    "while chatting:\n",
    "    chatting = await chat(kernel, chat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ragstack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
